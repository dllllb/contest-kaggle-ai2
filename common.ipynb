{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from elasticsearch import Elasticsearch, ElasticsearchException\n",
    "from zipfile import ZipFile\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from gensim import matutils\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_similarity(wv, ws1, ws2):\n",
    "    v1 = [v for v in [wv[word] for word in ws1 if word in wv] if v is not None]\n",
    "    v2 = [v for v in [wv[word] for word in ws2 if word in wv] if v is not None]\n",
    "\n",
    "    if v1 and v2:\n",
    "        return np.dot(matutils.unitvec(np.array(v1).mean(axis=0)), matutils.unitvec(np.array(v2).mean(axis=0)))\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StopwordTokenizer:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        clean_text = re.sub('[^a-zA-Z]', ' ', text)\n",
    "        words = clean_text.lower().split()\n",
    "        words = [w for w in words if w not in self.stopwords]\n",
    "        return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ir(df, index, fields, search_type):\n",
    "    es = Elasticsearch()\n",
    "\n",
    "    resutls = []\n",
    "    for ix, row in df.iterrows():\n",
    "        scores = {'id': ix}\n",
    "        for letter in ['A', 'B', 'C', 'D']:\n",
    "            query = {\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": row.question,\n",
    "                        \"type\": search_type,\n",
    "                        \"fields\": fields\n",
    "                    }\n",
    "                },\n",
    "                \"rescore\": {\n",
    "                    \"window_size\": 50,\n",
    "                    \"query\": {\n",
    "                        \"rescore_query\": {\n",
    "                            \"multi_match\": {\n",
    "                                \"query\": row['answer'+letter],\n",
    "                                \"type\": search_type,\n",
    "                                \"fields\": fields\n",
    "                            }\n",
    "                        },\n",
    "                        \"query_weight\": 0.5,\n",
    "                        \"rescore_query_weight\": 2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                hits = es.search(index=index, doc_type='doc,page', body=query, _source=True)['hits']['hits']\n",
    "                if len(hits) > 0:\n",
    "                    scores[letter] = hits[0]['_score']\n",
    "                else:\n",
    "                    scores[letter] = 0\n",
    "            except ElasticsearchException as e:\n",
    "                print(e)\n",
    "\n",
    "        resutls.append(scores)\n",
    "\n",
    "    return pd.DataFrame(resutls).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ir_rescore_sum(df, index, fields, search_type):\n",
    "    es = Elasticsearch()\n",
    "\n",
    "    resutls = []\n",
    "    for ix, row in df.iterrows():\n",
    "        scores = {'id': ix}\n",
    "        for letter in ['A', 'B', 'C', 'D']:\n",
    "            query = {\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": row.question,\n",
    "                        \"type\": search_type,\n",
    "                        \"fields\": fields\n",
    "                    }\n",
    "                },\n",
    "                \"rescore\": {\n",
    "                    \"window_size\": 50,\n",
    "                    \"query\": {\n",
    "                        \"rescore_query\": {\n",
    "                            \"multi_match\": {\n",
    "                                \"query\": row['answer'+letter],\n",
    "                                \"type\": search_type,\n",
    "                                \"fields\": fields\n",
    "                            }\n",
    "                        },\n",
    "                        \"query_weight\": 0.5,\n",
    "                        \"rescore_query_weight\": 2\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                hits = es.search(index=index, doc_type='doc,page', body=query, _source=False, size=50)['hits']['hits']\n",
    "                total_score = 0\n",
    "                for hit in hits:\n",
    "                    total_score += hit['_score']\n",
    "                scores[letter] = total_score\n",
    "            except ElasticsearchException as e:\n",
    "                print(e)\n",
    "\n",
    "        resutls.append(scores)\n",
    "\n",
    "    return pd.DataFrame(resutls).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ir_sum(df, index, fields, search_type):\n",
    "    es = Elasticsearch()\n",
    "\n",
    "    resutls = []\n",
    "    for ix, row in df.iterrows():\n",
    "        scores = {'id': ix}\n",
    "        for letter in ['A', 'B', 'C', 'D']:\n",
    "            query = {\n",
    "                \"query\": {\n",
    "                    \"multi_match\": {\n",
    "                        \"query\": row.question+' '+row['answer'+letter],\n",
    "                        \"type\": search_type,\n",
    "                        \"fields\": fields\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "\n",
    "            try:\n",
    "                hits = es.search(index=index, doc_type='doc,page', body=query, _source=False, size=50)['hits']['hits']\n",
    "                total_score = 0\n",
    "                for hit in hits:\n",
    "                    total_score += hit['_score']\n",
    "                scores[letter] = total_score\n",
    "            except ElasticsearchException as e:\n",
    "                print(e)\n",
    "\n",
    "        resutls.append(scores)\n",
    "\n",
    "    return pd.DataFrame(resutls).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.ck12.org/flx/show/epub/user:anBkcmVjb3VydEBnbWFpbC5jb20./Concepts_b_v8_vdt.epub\n",
    "def index_ck_12_conecpts_v8(index_name='ck12-concepts'):\n",
    "    es = Elasticsearch()\n",
    "\n",
    "    with ZipFile('Concepts_b_v8_vdt_html.zip') as zf:\n",
    "        es.indices.delete(index=index_name, ignore=[404])\n",
    "\n",
    "        index_settings = {\n",
    "            'settings': {\n",
    "                \"analysis\": {\n",
    "                    \"analyzer\": {\n",
    "                        \"default\": {\n",
    "                            # \"type\": \"snowball\",\n",
    "                            # \"language\": \"English\"\n",
    "                            \"type\": 'standard',\n",
    "                            \"stopwords\": \"_english_\"\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"index\": {\n",
    "                    \"number_of_shards\": 1,\n",
    "                    \"number_of_replicas\": 0\n",
    "                },\n",
    "            },\n",
    "            \"mappings\": {\n",
    "                \"doc\": {\n",
    "                    \"properties\": {\n",
    "                        \"text\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"similarity\": \"BM25\"\n",
    "                        },\n",
    "                        \"title\": {\n",
    "                            \"type\": \"string\",\n",
    "                            \"similarity\": \"BM25\"\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "        es.indices.create(index=index_name, body=index_settings)\n",
    "\n",
    "        for name in zf.namelist():\n",
    "            with zf.open(name) as f:\n",
    "                bs = BeautifulSoup(f, 'lxml')\n",
    "                tag = bs.find('h1')\n",
    "                title = tag.text.strip()\n",
    "                text = ''\n",
    "                while True:\n",
    "                    tag = tag.find_next_sibling()\n",
    "                    if tag is None or tag.name == 'h1':\n",
    "                        doc = {\n",
    "                            'title': title,\n",
    "                            'text': text\n",
    "                        }\n",
    "                        es.index(index=index_name, doc_type='doc', body=doc)\n",
    "                        if tag is None:\n",
    "                            break\n",
    "                        else:\n",
    "                            title = tag.text.strip()\n",
    "                            text = ''\n",
    "                    else:\n",
    "                        text += tag.text\n",
    "\n",
    "        es.indices.optimize(index=index_name, max_num_segments=1, wait_for_merge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_we(df, word2vec):\n",
    "    resutls = []\n",
    "    tokenizer = StopwordTokenizer()\n",
    "    for ix, row in df.iterrows():\n",
    "        q_tokens = set(tokenizer.tokenize(row.question))\n",
    "        scores = {'id': ix}\n",
    "        for letter in ['A', 'B', 'C', 'D']:\n",
    "            a_tokens = set(tokenizer.tokenize(row['answer'+letter]))\n",
    "            sim = n_similarity(word2vec, q_tokens, a_tokens)\n",
    "            scores[letter] = sim\n",
    "\n",
    "        resutls.append(scores)\n",
    "\n",
    "    return pd.DataFrame(resutls).set_index('id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# download a dataset from [GloVe](http://nlp.stanford.edu/projects/glove/)\n",
    "# add a header in form of N_WORDS space VECTOR_LEN to the beginning of the file\n",
    "class GloveEstimator(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, vectors):\n",
    "        self.vectors = vectors\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.word2vec = Word2Vec.load_word2vec_format(self.vectors, binary=False)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = predict_we(X, self.word2vec)\n",
    "        return pred.div(pred.max(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrEstimator(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, index, fields=None, search_type='most_fields'):\n",
    "        self.index = index\n",
    "        self.fields = fields or ['_all']\n",
    "        self.search_type = search_type\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = predict_ir(X, index=self.index, fields=self.fields, search_type=self.search_type)\n",
    "        return pred.div(pred.max(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrEstimatorRescoreSum(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, index, fields=None, search_type='most_fields'):\n",
    "        self.index = index\n",
    "        self.fields = fields or ['_all']\n",
    "        self.search_type = search_type\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = predict_ir_rescore_sum(X, index=self.index, fields=self.fields, search_type=self.search_type)\n",
    "        return pred.div(pred.max(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrEstimatorSum(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, index, fields=None, search_type='most_fields'):\n",
    "        self.index = index\n",
    "        self.fields = fields or ['_all']\n",
    "        self.search_type = search_type\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        pred = predict_ir_sum(X, index=self.index, fields=self.fields, search_type=self.search_type)\n",
    "        return pred.div(pred.max(axis=1), axis=0).fillna(0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.predict_proba(X).idxmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
